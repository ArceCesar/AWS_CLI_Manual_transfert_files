{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266442be",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Script to transfer manually using AWS CLI (Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e104e",
   "metadata": {},
   "source": [
    "## **Table of Contents :<a name=\"return\"></a>**\n",
    "1. [Move All Files Inbound to Archive - Dev using AWS CLI](#move_files_dev)\n",
    "1. [Move All Files Inbound to Archive - QA using AWS CLI](#move_files_qa)\n",
    "1. [Copy Files QA to Dev using AWS CLI (Inbound - CDS)](#move_files_CDS_qa_dev)\n",
    "1. [Copy Files QA to Dev using AWS CLI (Inbound - MDM)](#move_files_MDM_qa_dev)\n",
    "1. [Remove Buckets using AWS CLI](#remove_buckets)\n",
    "1. [Create Treeview mapping Dev using AWS CLI (Archive)](#create_treeview_files_dev_archive)\n",
    "1. [Create Treeview mapping Dev using AWS CLI (Inbound)](#create_treeview_files_dev_inbound)\n",
    "1. [Create Treeview mapping Staging using AWS CLI (Archive)](#create_treeview_files_staging_archive)\n",
    "1. [Create Treeview mapping Staging using AWS CLI (Inbound)](#create_treeview_files_staging_inbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f6080",
   "metadata": {},
   "source": [
    "#### --> Target: Create a TEXT File to move from Source to Target using AWS CLI <br>\n",
    "aws s3 mv  s3://agropur-global-nonprod-account-dev-sapco/SAPCO/Inbound/Financials/Costing/ \n",
    "   s3://agropur-global-nonprod-account-dev-sapco/SAPCO/Archive/Financials/Costing/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf015a3c",
   "metadata": {},
   "source": [
    "## Move All Files from Inbound to Archive (Dev) <a name=\"move_files_dev\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3d231",
   "metadata": {},
   "source": [
    "Target : \"s3://agropur-global-nonprod-account-dev-sapco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df555f57",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-dev-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "dev_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "new_dev_source= dev_source[['source','target']]\n",
    "\n",
    "with open('Move_Files_Dev_Inbound_to_Archive.txt', 'w') as f:\n",
    "    for i in range(len(new_dev_source)):\n",
    "        line = \"aws s3 mv\"+ \" \"+ bucketname + new_dev_source.loc[i, \"source\"] + \" \" + bucketname \\\n",
    "        + new_dev_source.loc[i, \"target\"] + \" --recursive\"\n",
    "        f.write(line + \"\\n\")\n",
    "        \n",
    "with open('Move_Files_Dev_Inbound_to_Archive.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "        \n",
    "for i in range(len(new_dev_source)):\n",
    "    print('aws s3 mv ', bucketname, new_dev_source.loc[i, \"source\"], ' ', bucketname, new_dev_source.loc[i, \"target\"], ' --recursive', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545d698",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d93bf",
   "metadata": {},
   "source": [
    "## To Move All Files from Inbound to Archive (QA) <a name=\"move_files_qa\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bfe51",
   "metadata": {},
   "source": [
    "Target : \"s3://agropur-global-nonprod-account-staging-sapco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62495075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-staging-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "staging_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "new_staging_source= staging_source[['source','target']]\n",
    "\n",
    "with open('Move_Files_QA_Inbound_to_Archive.txt', 'w') as f:\n",
    "    for i in range(len(new_staging_source)):\n",
    "        line = \"aws s3 mv\"+ \" \"+ bucketname + new_staging_source.loc[i, \"source\"] + \" \" + bucketname \\\n",
    "        + new_dev_source.loc[i, \"target\"] + \" --recursive\"\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Move_Files_QA_Inbound_to_Archive.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(new_staging_source)):\n",
    "    print('aws s3 mv ', bucketname, new_staging_source.loc[i, \"source\"], ' ', bucketname, new_dev_source.loc[i, \"target\"], ' --recursive', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d66656",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed46542",
   "metadata": {},
   "source": [
    "## Copy All Files CDS from Staging QA to Staging Dev <a name=\"move_files_CDS_qa_dev\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-dev-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "staging_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23_CDS')\n",
    "new_staging_source= staging_source[['source_staging_path','source_dev_path']]\n",
    "\n",
    "with open('Copy_CDS_Files_Inbound_QA_to_Dev.txt', 'w') as f:\n",
    "    for i in range(len(new_staging_source)):\n",
    "        line = \"aws s3 cp\"+ \" \"+ new_staging_source.loc[i, \"source_staging_path\"] + \" \" + new_staging_source.loc[i, \"source_dev_path\"] + \" --recursive\"\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Copy_CDS_Files_Inbound_QA_to_Dev.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(new_staging_source)):\n",
    "    print('aws s3 cp ', new_staging_source.loc[i, \"source_staging_path\"], ' ', new_staging_source.loc[i, \"source_dev_path\"], ' --recursive', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed5146",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48abc6",
   "metadata": {},
   "source": [
    "## Copy All Files MDM from Staging QA to Staging Dev <a name=\"move_files_MDM_qa_dev\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc39823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-dev-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "staging_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23_MDM')\n",
    "new_staging_source= staging_source[['source_staging_path','source_dev_path']]\n",
    "\n",
    "with open('Copy_MDM_Files_Inbound_QA_to_Dev.txt', 'w') as f:\n",
    "    for i in range(len(new_staging_source)):\n",
    "        line = \"aws s3 cp\"+ \" \"+ new_staging_source.loc[i, \"source_staging_path\"] + \" \" + new_staging_source.loc[i, \"source_dev_path\"] + \" --recursive\"\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Copy_MDM_Files_Inbound_QA_to_Dev.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(new_staging_source)):\n",
    "    print('aws s3 cp ', new_staging_source.loc[i, \"source_staging_path\"], ' ', new_staging_source.loc[i, \"source_dev_path\"], ' --recursive', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b553b0d",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df86c5",
   "metadata": {},
   "source": [
    "# Remove Bucket using CLI <a name=\"remove_buckets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f214d9",
   "metadata": {},
   "source": [
    "aws s3 rb s3://mybucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b085458",
   "metadata": {},
   "source": [
    "Erase a Folder in a bucket: (WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53420d",
   "metadata": {},
   "source": [
    "### Remove Bucket Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f637d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 rm s3://agropur-global-nonprod-account-dev-sapco/SAPCO/Inbound/ --recursive\n",
    "# aws s3 rm s3://agropur-global-nonprod-account-dev-sapco/SAPCO/Archive/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86aea32",
   "metadata": {},
   "source": [
    "### Remove Bucket Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17584480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 rm s3://agropur-global-nonprod-account-staging-sapco/SAPCO/Inbound/ --recursive\n",
    "# aws s3 rm s3://agropur-global-nonprod-account-staging-sapco/SAPCO/Archive/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f8ec9",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953288f2",
   "metadata": {},
   "source": [
    "## Create Treeview files - dev / Archive <a name=\"create_treeview_files_dev_archive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-dev-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "dev_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "dev_archive_source= dev_source[['target']]\n",
    "\n",
    "with open('Create_mapping_Dev_Archive.txt', 'w') as f:\n",
    "    for i in range(len(dev_archive_source)):\n",
    "        line = \"aws s3 cp .\" + \" \" + bucketname + dev_archive_source.loc[i, \"target\"] + \" \" + '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\"--include \".bash_history\"' \n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Create_mapping_Dev_Archive.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "\n",
    "for i in range(len(dev_archive_source)):\n",
    "    print('aws s3 cp . ', bucketname, dev_archive_source.loc[i, \"target\"], ' ', '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\" --include \".bash_history\"', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c20be0",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af353541",
   "metadata": {},
   "source": [
    "## Create Treeview files - dev / Inbound <a name=\"create_treeview_files_dev_inbound\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-dev-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "dev_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "dev_inbound_source= dev_source[['source']]\n",
    "\n",
    "with open('Create_mapping_Dev_Inbound.txt', 'w') as f:\n",
    "    for i in range(len(dev_inbound_source)):\n",
    "        line = \"aws s3 cp .\" + \" \" + bucketname + dev_inbound_source.loc[i, \"source\"] + \" \" + '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\"--include \".bash_history\"' \n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Create_mapping_Dev_Inbound.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(dev_inbound_source)):\n",
    "    print('aws s3 cp . ', bucketname, dev_inbound_source.loc[i, \"source\"], ' ', '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\" --include \".bash_history\"', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9bcf1",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ace80",
   "metadata": {},
   "source": [
    "## Create Treeview files - staging / Inbound <a name=\"create_treeview_files_staging_inbound\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-staging-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "staging_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "staging_inbound_source= staging_source[['source']]\n",
    "\n",
    "with open('Create_mapping_Staging_Inbound.txt', 'w') as f:\n",
    "    for i in range(len(staging_inbound_source)):\n",
    "        line = \"aws s3 cp .\" + \" \" + bucketname + staging_inbound_source.loc[i, \"source\"] + \" \" + '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\"--include \".bash_history\"' \n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Create_mapping_Staging_Inbound.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(staging_inbound_source)):\n",
    "    print('aws s3 cp . ', bucketname, staging_inbound_source.loc[i, \"source\"], ' ', '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\" --include \".bash_history\"', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ad322",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae84d1",
   "metadata": {},
   "source": [
    "## Create Treeview files - staging / Archive <a name=\"create_treeview_files_staging_archive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucketname = \"s3://agropur-global-nonprod-account-staging-sapco\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucketname)\n",
    "staging_source = pd.read_excel(\"AWS_S3_Treeview_mapping.xlsx\", sheet_name='14Apr23')\n",
    "staging_archive_source= staging_source[['target']]\n",
    "\n",
    "with open('Create_mapping_Staging_Archive.txt', 'w') as f:\n",
    "    for i in range(len(staging_archive_source)):\n",
    "        line = \"aws s3 cp .\" + \" \" + bucketname + staging_archive_source.loc[i, \"target\"] + \" \" + '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\"--include \".bash_history\"' \n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open('Create_mapping_Staging_Archive.txt', 'a') as file:\n",
    "    file.write('aws s3 ls ' + bucketname + '\\n')\n",
    "    \n",
    "for i in range(len(staging_archive_source)):\n",
    "    print('aws s3 cp . ', bucketname, staging_archive_source.loc[i, \"target\"], ' ', '--region us-east-2 --recursive --exclude \".ba*\" --exclude \"*.*\" --include \".bash_history\"', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5402f",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080edde",
   "metadata": {},
   "source": [
    "### Remove a file (Exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57b43e",
   "metadata": {},
   "source": [
    "aws s3 rm s3://agropur-global-nonprod-account-dev-sapco/SAPCO/Archive/TEST_MASTERDATA_FIN/.bash_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78ae61",
   "metadata": {},
   "source": [
    "[[Move Up]](#return)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
